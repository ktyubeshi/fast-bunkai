from __future__ import annotations

import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Iterable, List, Tuple

import pytest
from bunkai import Bunkai

from fast_bunkai import FastBunkai

SAMPLE_TEXTS = [
    "ã“ã‚“ã«ã¡ã¯ã€‚ã‚ã‚ŠãŒã¨ã†ã€‚",
    "ã‚¹ã‚¿ãƒƒãƒ•? ã¨è©±ã—è¾¼ã¿ã€‚",
    "No.1ã®ãƒ›ãƒ†ãƒ«ã§ã™ã€‚",
    "ä¾¡æ ¼ã¯3.5ä¸‡å††ã§ã™ã€‚",
    "ãƒ¡ãƒ¼ãƒ«ã¯test@example.comã§ã™ã€‚",
    "é¡”æ–‡å­—(*^_^*)ã ã‚ˆã€‚",
    "***(*^_^*)ã ã‚ˆã€‚",
    "ã‚„ã£ãŸãƒ¼(å¬‰)ï¼",
    "ã‚ãƒ¼ã„â€¦ï¼",
    "ROOM No.411ã§ã—ãŸã€‚",
    "åˆå®¿å…è¨±? ã®è‹¥è€…ã•ã‚“é”ã§ã—ã‚‡ã†ã‹",
    "ã‚¹ã‚¿ãƒƒãƒ•? ã¨è©±ã—è¾¼ã¿\næ¬¡ã®è¡Œã§ã™ã€‚",
]


EDGE_CASE_TEXTS = [
    "",
    "   ",
    "\n\n\n",
    "ã€‚ã€‚ã€‚",
    "ðŸ‘ðŸ‘ðŸ‘",
    "A.B.C",
    "ãŠã¯ã‚ˆã†ðŸŒžã”ã–ã„ã¾ã™ï¼ï¼",
    "çµ‚ç«¯è¨˜å·...\n\næ¬¡ã®æ®µè½ã€‚",
    "ðŸ˜€\ufe0f test .",
    "First sentenceçµ‚ã‚ã‚Š\nSecond sentenceç¶šãã€‚",
]


PIPELINE_FOCUSED_TEXTS = [
    "é¡”æ–‡å­—(ãƒŽÂ´âˆ€`*)ã‚ã‚ŠãŒã¨ã†ã€‚ã™ãè¿”ä¿¡ã™ã‚‹ã­ã€‚",
    "é€Ÿå ±ðŸš€âœ¨Python3.13ãŒæ¥ãŸï¼Rustã‚‚è¿½éšäºˆå®šï¼",
    "å½¼ã¯ã€Žã‚„ã‚‹ã£ã¦ã€ã¨ç¬‘ã£ã¦ã„ãŸã€‚æ˜Žæ—¥ã‚‚æ¥ã‚‹ã‚‰ã—ã„ã€‚",
    "ãƒãƒ¼ã‚¸ãƒ§ãƒ³v1.2.3ã‚’é…å¸ƒä¸­ã€‚ã‚µãƒ¼ãƒãƒ¼ã¯127.0.0.1ã§ã™ã€‚",
    "å¾—ç‚¹ã¯12.5ç‚¹ã ã£ãŸãŒã€No.10ã®é¸æ‰‹ãŒé€†è»¢ã—ãŸã€‚",
    "å¼•ç”¨æ–‡? ã¨ã„ã†è³ªå•ã«å›°ã£ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§è­°è«–ã—ã‚ˆã†ã€‚",
    "Twitteré¢¨æŠ•ç¨¿: PythonðŸã§æ›¸ã„ã¦RustðŸ¦€ã§æœ€é©åŒ–ã™ã‚‹ã‚ˆï¼ #dev #fast",
    "æ”¹è¡Œãƒ†ã‚¹ãƒˆ\n\nã“ã“ã§å¼·åˆ¶çš„ã«åŒºåˆ‡ã‚‹ã€‚ã•ã‚‰ã«ç¶šãã€‚",
]


def _load_fixture_texts() -> List[str]:
    data_dir = Path(__file__).parent / "data" / "texts"
    if not data_dir.is_dir():
        return []
    return [path.read_text(encoding="utf-8") for path in sorted(data_dir.glob("*.txt"))]


FILE_TEXTS: Tuple[str, ...] = tuple(_load_fixture_texts())


ALL_TEXTS: Tuple[str, ...] = tuple(
    SAMPLE_TEXTS + EDGE_CASE_TEXTS + PIPELINE_FOCUSED_TEXTS + list(FILE_TEXTS)
)


def collect_sentence_boundaries(ann) -> List[int]:
    return sorted({span.end_index for span in ann.get_final_layer()})


def collect_morph_surfaces(ann) -> List[str]:
    if "MorphAnnotatorJanome" not in ann.available_layers():
        return []
    surfaces: List[str] = []
    seen = set()
    for span in ann.get_annotation_layer("MorphAnnotatorJanome"):
        token = span.args.get("token") if span.args else None  # type: ignore[assignment]
        if token is None:
            continue
        surface = getattr(token, "word_surface", None)
        if not surface:
            continue
        key = (span.start_index, span.end_index, surface)
        if key in seen:
            continue
        seen.add(key)
        surfaces.append(surface)
    return surfaces


@pytest.mark.parametrize("text", ALL_TEXTS)
def test_sentence_splitting_matches_bunkai(text: str) -> None:
    fast = FastBunkai()
    ref = Bunkai()

    assert list(fast(text)) == list(ref(text))
    assert fast.find_eos(text) == ref.find_eos(text)

    fast_ann = fast.eos(text)
    ref_ann = ref.eos(text)

    assert sorted(fast_ann.available_layers()) == sorted(ref_ann.available_layers())
    assert collect_sentence_boundaries(fast_ann) == collect_sentence_boundaries(ref_ann)
    assert collect_morph_surfaces(fast_ann) == collect_morph_surfaces(ref_ann)


def _sequential_reference(texts: Iterable[str]) -> List[List[str]]:
    ref = Bunkai()
    return [list(ref(text)) for text in texts]


def test_multithread_consistency() -> None:
    texts = list(ALL_TEXTS) * 5
    fast = FastBunkai()

    with ThreadPoolExecutor(max_workers=8) as executor:
        results = list(executor.map(lambda t: list(fast(t)), texts))

    assert results == _sequential_reference(texts)


@pytest.mark.asyncio
async def test_asyncio_consistency() -> None:
    texts = list(ALL_TEXTS) * 3
    fast = FastBunkai()

    loop = asyncio.get_running_loop()

    async def run(text: str) -> List[str]:
        return await loop.run_in_executor(None, lambda: list(fast(text)))

    results = await asyncio.gather(*(run(text) for text in texts))
    assert results == _sequential_reference(texts)
