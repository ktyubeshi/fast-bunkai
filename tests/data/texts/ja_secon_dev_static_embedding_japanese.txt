# 100倍速で実用的な文章ベクトルを作れる、日本語StaticEmbeddingモデルを公開

文章の密ベクトルは、情報検索・文章判別・類似文章抽出など、さまざまな用途に使うことができます。しかしながら最先端のTransformerモデルは小さいモデルでも、とりわけCPU環境では処理速度が遅いため実用でないこともしばしばあります。

この課題を解決する新しいアプローチとして、先日公開されたTransformerモデル「ではない」StaticEmbeddingモデルは、例えばintfloat/multilingual-e5-small(以下mE5-small)とのベンチマーク比較では85%のスコアという最低十分な性能で、何よりCPUで動作時に126倍高速に文ベクトルを作成することができる、という驚きの速度です。

というわけで、早速日本語(と英語)で学習させたモデルsentence-embedding-japaneseを作成し、公開しました。

日本語の文章ベクトルの性能を評価するJMTEBの結果によると、総合スコアではmE5-smallには若干及ばないまでも、タスクによっては勝っていたりしますし、他の日本語baseサイズbertモデルよりもスコアが高いこともあるぐらい、最低限実用できそうな性能が出ています。本当にそんなに性能が出るのか実際に学習させてみるまでは半信半疑でしたが、驚きです。

なお、StaticEmbedding日本語モデル学習などの技術的なことは記事の後半に書いているので、興味がある方はどうぞ。

## 利用方法

利用は簡単、SentenceTransformerを使っていつもの方法で文章ベクトルを作れます。今回はGPUを使わず、CPUで実行してみましょう。なおSentenceTransformerは3.3.1で試しています。

文章ベクトルを作成すると、queryにマッチする文章のスコアが高くなるように計算できています。この例文では、例えばBM25ではqueryに含まれる「ラーメン」のような直接的な単語が文章に出ていないため、うまくマッチさせることが難しいでしょう。

類似文章タスクの例でも、類似文章が高スコアになる結果になりました。

またTransformerモデルを利用してCPUで文章ベクトルを作った場合、少ない文章量でもだいぶ時間がかかる、という経験をされた方も多いと思います。StaticEmbeddingモデルではCPUがそこそこ速ければ一瞬で終わるはず。さすが100倍速。

### 出力次元を小さくする

標準で作られる文ベクトルの次元は1024ですが、これをさらに小さく次元削減することもできます。例えば128を指定することができます。truncate_dimは32、64、128、256、512、1024から指定可能です。

128次元のベクトルになり、結果のスコアも若干変わります。次元が小さくなったことで、性能が少々劣化しています(後半にベンチマークを記載)。ただ1024次元から128次元に減ることで、保存するストレージサイズが減ったり、検索時などに利用する類似度計算コストが約8倍速になったりと、用途によっては小さい次元の方が嬉しいことも多いでしょう。

## なぜCPUで推論が高速なの?

StaticEmbeddingはTransformerモデルではありません。つまりTrasformerの特徴である「Attention Is All You Need」なアテンションの計算が一切ないのです。文章に出てくる単語トークンを1024次元のテーブルに保存して、文ベクトル作成時にはそれの平均をとっているだけです。なお、アテンションがないので、文脈の理解などはしていません。

また内部実装ではPyTorchのnn.EmbeddingBagを使って、全てを連結したトークンとオフセットを渡して処理することで、PyTorchの最適化で高速なCPU並列処理とメモリアクセスがされているようです。

元記事の速度評価結果によるとCPUではmE5-smallと比べて126倍速らしいですね。

## 評価結果

JMTEBでの全ての評価結果はJSONファイルに記載しています。JMTEB Leaderboardで他のモデルと見比べると、相対的な差がわかるでしょう。JMTEBの全体の評価結果はモデルサイズを考えると、すこぶる良好です。なお、JMTEBのmr-tidyタスクは700万文章のベクトル化を行うので処理に時間がかなりかかる(モデルにもよりますがRTX4090で1〜4時間ほど)と思います。これもStaticEmbeddingsでは非常に速く、RTX4090では約4分で処理終えることができました。

### 情報検索でBM25の置き換えができそうか?

JMTEBの中の情報検索タスクのRetrievalの結果を見てみましょう。StaticEmbeddingではmr-tidyの項目が著しく悪いですね。mr-tidyは他のタスクに比べて文章量が圧倒的に多く(700万文章)、つまる所大量の文章を検索するようなタスクでは結果が悪い可能性がありそうです。文脈を無視した単純なトークンの平均なので、増えれば増えるほど似た平均の文章が出てくるとすると、そういう結果にもなり得そうですね。

ので、大量の文章の場合、BM25よりもだいぶ性能が悪い可能性がありそうです。ただ、少ない文章で、ずばりの単語マッチが少ない場合は、BM25よりも良好な結果になることが多そうですね。

なお情報検索タスクのjaqketの結果が他のモデルに対してやたら良いのは、jaqketの問題を含むJQaRa(dev、unused)を学習しているからといっても、高すぎる感じで謎です。testの情報リークはしていないとは思うのですが…。

### クラスタリング結果が悪い

こちらも詳細は追っかけていませんが、スコア的には他のモデルよりもだいぶ悪い結果ですね。クラス分類タスクは悪くないので不思議です。埋め込み空間がマトリョーシカ表現学習で作られた影響もあるのでしょうか。

## JQaRA、JaCWIRでのリランキングタスク評価

JQaRa評価はBM25よりは若干良く、mE5-smallよりは若干低い、JaCWIRはBM25、mE5よりだいぶ低い感じの結果になりました。

JaCWIRはqueryから探しあてる文章が、Web文章のタイトルと概要文なので、いわゆる「綺麗な」文章ではないケースも多いです。transformerモデルはノイズに強いので、単純なトークン平均のStaticEmbeddingではスコアに差がつけられるのも納得ですね。BM25は特徴的な単語が出現した文章にマッチするので、JaCWIRでもノイズとなるような文章上の単語はクエリにそもそもマッチしないため、Transformerモデルと競争力のある結構良い結果を残しています。

この結果から、StaticEmbeddingはTransformer/BM25に比べ、ノイズを多く含む文章の場合はスコアが悪い可能性があります。

## 出力次元の削減

StaticEmbeddingで出力される次元は、学習次第ですが今回作成したものは1024次元とそこそこのサイズです。次元数が大きいと、推論後のタスク(クラスタリングや情報検索など)に計算コストがかかってしまいます。しかしながら、学習時にマトリョーシカ表現学習(Matryoshka Representation Learning, MRL)をしているため、1024次元をさらに小さな次元へと簡単に次元削減ができます。

MRLは、学習時に先頭のベクトルほど重要な次元を持ってくることで、例えば1024次元でも先頭の32、64、128、256次元だけを使って後ろを切り捨てるだけで、ある程度良好な結果を示しています。

StaticEmbeddingの記事によると、128次元で91.87%、256次元で95.79%、512次元で98.53%の性能を維持しているようです。精度にそこまでシビアではないが、その後の計算コストを下げたい場合、ガッと次元削減して使う、という用途にも使えそうですね。

### StaticEmbdding日本語モデルでの次元削減結果

JMTEBでは、出力時にモデルのパラメータを制御できるため、truncate_dimオプションを渡すことで、次元削減した結果のベンチマークも簡単に計測できます。素晴らしいですね。というわけで、StaticEmbdding日本語モデルでも、次元削減した結果でベンチマークをとってみました。

512次元でのスコア計測が間違っていたので修正しました。マトリョーシカ表現学習がうまく反映され、次元数を削ると若干のスコア低下が見られますが、次元数が減ったためその後のコストが抑えられそうですね。

クラスタリングタスクにおいては128次元まで次元削減しても1024次元よりもスコアが高い、という本来情報量を削らない方がスコアが良くなりそうなのに、クラスタリングタスクのみは逆にスコアが上がってしまう興味深い結果となりました。マトリョーシカ表現学習では、先頭の次元の方が全体的な特徴を踏まえているので、クラスタリング用途には(クラスタリングのアルゴリズムにもよると思いますが)、特徴的な前の方の次元のみで後ろの次元を使わない方が良質な結果が得られる、ということなのかもしれません。

というわけで、static-embedding-japaneseモデルで次元削減する時は、512、256、128次元あたりが性能と次元削減のバランスが取れてそうですね。

## StaticEmbeddingモデルを作ってみて

正直、単純なトークンのembeddingsの平均でそんなに性能出るのか半信半疑だったのですが、実際に学習させてみてシンプルなアーキテクチャなのに性能の高さにびっくりしました。Transformer全盛のこの時代に、古き良き単語埋め込みの活用モデルで、実世界で利活用できそうなモデルの出現に驚きを隠せません。

CPUでの推論速度が速い文ベクトル作成モデルは、ローカルCPU環境で大量の文章の変換などはもとより、エッジデバイスだったりネットワークが遅い(リモートの推論サーバを叩けない)環境だったり、色々と活用できそうですね。

## StaticEmbedding日本語モデル学習のテクニカルノート

### なぜうまく学習できるのか

StaticEmbeddingは非常にシンプルで、文章をトークナイズしたIDで単語の埋め込みベクトルが格納されているEmbeddingBagテーブルからN次元(今回は1024次元)のベクトルを取得し、その平均を取るだけです。

これまで、単語埋め込みベクトルといえば、word2vecやGloVeのように、Skip-gramやCBOWを用いて単語の周辺を学習してきました。しかし、StaticEmbeddingでは文章全体を用いて学習しています。また、対照学習を使って大量の様々な文章を巨大バッチで学習しており、良い単語の埋め込み表現の学習に成功しています。

対照学習は、基本的に正例以外全てを負例として学習するため、例えばバッチサイズ2048なら1の正例に対して2047の負例を2048通り、つまり2048×2047で約400万の比較を学習します。そのため、元の単語空間に対して適切な重みを更新しながら、学習を進めることができるのです。

### 学習データセット

日本語モデル学習にあたり、対照学習で利用できるデータセットとして、hotchpotch/sentence_transformer_japaneseを作成し使用しました。これはSentenceTransformerで学習しやすいカラム名と構造に整えたものです。

このデータセットは、複数の既存データセットを基に作成しました。毎度ながらデータセットの作者の方々、とりわけhpprc氏に感謝です。

上記の作成したデータセットの中で、複数のサブセットを使用しました。なお、情報検索を強化したかったため、情報検索に適したデータセットのデータはオーギュメンテーションで件数を多めに学習させています。

英語データセットには、sentence-transformersやnthakurが公開している複数のデータセットを利用しています。

### 日本語トークナイザ

StaticEmbeddingを学習するためには、HuggingFaceのトークナイザライブラリのtokenizer.json形式で処理可能なトークナイザを使うと簡単そうだったので、hotchpotch/xlm-roberta-japanese-tokenizerというトークナイザを作成しました。語彙数は32,768です。

このトークナイザは、wikipedia日本語(訂正:作成コードを確認したところ、wikipedia日本語のみを利用していました)のデータをunidicで分割し、sentencepiece unigramで学習したものです。XLM-Roberta形式の日本語トークナイザとしても機能します。今回はこのトークナイザを利用しました。

### ハイパーパラメータ

大元の学習コードとの変更点やメモは以下の通りです。

batch_sizeを大元の2048から6072に設定しました。対照学習で巨大なバッチを処理するとき、同一バッチ内にポジティブとネガティブが含まれると学習に悪影響を与える可能性があります。これを防ぐためにBatchSamplers.NO_DUPLICATESオプションがあります。しかし、バッチサイズが巨大だと同一バッチに含めないためのサンプリング処理に時間がかかることがあります。今回はBatchSamplers.NO_DUPLICATESを指定し、RTX4090の24GBに収まる6072に設定しました。バッチサイズはさらに大きい方が結果が良い可能性があります。

epoch数を1から2に変更しました。1よりも2の方が良い結果になりました。ただし、データサイズがもっと大きければ、1の方が良い可能性があります。

スケジューラは標準のlinearから、経験則でより良いと感じるcosineに変更しました。

オプティマイザは標準のAdamWのままです。adafactorに変更した場合、収束が悪くなりました。

learning_rateは2e-1のままです。値が巨大すぎるのではないかと疑問に思いましたが、低くすると結果が悪化しました。

dataloader_prefetch_factorを4、dataloader_num_workersを15に設定しました。トークナイズとバッチサンプラのサンプリングに時間がかかるため、大きめに設定しました。

### 学習リソース

CPUはRyzen9 7950X、GPUはRTX4090、メモリは64GBを使用しました。このマシンリソースで、フルスクラッチ学習にかかった時間は約4時間でした。GPUのコア負荷は非常に小さく、他のtransformerモデルでは学習時に90%前後で張り付くのに対して、StaticEmbeddingではほとんど0%でした。これは、巨大なバッチをGPUメモリに転送する時間が大半を占めているためかと思われます。そのため、GPUメモリの帯域幅が速くなれば、学習速度がさらに向上する可能性があります。

### さらなる性能向上へ

今回利用したトークナイザはStaticEmbedding向けに特化したものではないため、より適したトークナイザを使用すれば性能が向上する可能性があります。バッチサイズをさらに巨大化することで、学習の安定性が向上し、性能向上が見込めるかもしれません。

また、さまざまなドメインや合成データセットを利用するなど、より幅広い文章リソースを学習に組み込むことで、さらなる性能向上が期待できます。

### ライセンス

static-embedding-japaneseはモデル重み・学習コードをMITライセンスで公開しています。
